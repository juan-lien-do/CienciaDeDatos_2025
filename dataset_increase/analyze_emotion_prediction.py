from neo4j import GraphDatabase
import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import cross_val_score, StratifiedKFold
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve
import xgboost as xgb
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter
import warnings
warnings.filterwarnings('ignore')

# Configuraci√≥n de Neo4j
NEO4J_URI = "neo4j://127.0.0.1:7687"
NEO4J_USER = "neo4j"
NEO4J_PASSWORD = "tu_password"  # CAMBIAR POR TU PASSWORD

def analyze_emotion_prediction():
    """
    Analiza si el an√°lisis de emociones puede predecir la viralidad de noticias.
    
    Flujo:
    1. Extrae datos del conjunto de entrenamiento desde Neo4j
    2. Prepara features de emociones con one-hot encoding
    3. Entrena modelo XGBoost para predecir viralidad
    4. Eval√∫a rendimiento con validaci√≥n cruzada
    5. Analiza importancia de features emocionales
    """
    
    print("üî¨ AN√ÅLISIS DE PREDICCI√ìN DE VIRALIDAD CON EMOCIONES")
    print("=" * 80)
    print("üéØ Objetivo: Evaluar capacidad predictiva de emociones para viralidad")
    print("ü§ñ Modelo: XGBoost con one-hot encoding")
    print("üìä Conjunto: Solo datos de entrenamiento (subset='train')")
    print("üé≠ Features: Emociones + Sentimientos + Scores")
    print("=" * 80)
    
    # Cargar datos desde Neo4j
    df_train = load_training_data()
    
    if df_train.empty:
        print("‚ùå No se encontraron datos de entrenamiento")
        return
    
    # Preparar features y target
    X, y, feature_names = prepare_features_and_target(df_train)
    
    # Entrenar y evaluar modelo
    model_results = train_and_evaluate_model(X, y, feature_names)
    
    # An√°lisis de importancia de features
    analyze_feature_importance(model_results, feature_names, df_train)
    
    # An√°lisis estad√≠stico de emociones por viralidad
    analyze_emotion_statistics(df_train)
    
    print("\n" + "=" * 80)
    print("‚úÖ AN√ÅLISIS COMPLETADO")
    print("üìà Revisa los gr√°ficos generados para insights detallados")
    print("üí° Conclusiones disponibles en el reporte final")
    print("=" * 80)


def load_training_data():
    """
    Carga datos de entrenamiento con an√°lisis de emociones desde Neo4j.
    
    Returns:
        pd.DataFrame: Datos del conjunto de entrenamiento
    """
    print("\nüìä Cargando datos de entrenamiento desde Neo4j...")
    
    driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASSWORD))
    
    try:
        with driver.session() as session:
            # Query para obtener datos de train con emociones y sentimientos (del 10001 al 39610)
            query = """
                MATCH (n:Noticia)
                WHERE n.subset = 'train'
                  AND n.popularity IS NOT NULL
                  AND n.analisis_sentimiento_titulo_label IS NOT NULL
                  AND n.analisis_emocion_titulo_label IS NOT NULL
                RETURN 
                    n.titulo AS titulo,
                    n.shares AS shares,
                    n.popularity AS is_viral,
                    n.analisis_sentimiento_titulo_label AS sentiment_label,
                    n.analisis_sentimiento_titulo_score AS sentiment_score,
                    n.analisis_sentimiento_titulo_all_labels AS sentiment_all_labels,
                    n.analisis_sentimiento_titulo_all_scores AS sentiment_all_scores,
                    n.analisis_emocion_titulo_label AS emotion_label,
                    n.analisis_emocion_titulo_score AS emotion_score,
                    n.analisis_emocion_titulo_all_labels AS emotion_all_labels,
                    n.analisis_emocion_titulo_all_scores AS emotion_all_scores
            """
            
            result = session.run(query)
            records = [dict(record) for record in result]
            
            if not records:
                print("   ‚ùå No se encontraron datos con an√°lisis completo")
                return pd.DataFrame()
            
            df = pd.DataFrame(records)
            
            print(f"   ‚úÖ Cargados {len(df)} registros de entrenamiento")
            print(f"   üìà Distribuci√≥n de viralidad:")
            viral_counts = df['is_viral'].value_counts()
            for viral, count in viral_counts.items():
                percentage = count / len(df) * 100
                status = "Viral" if viral else "No Viral"
                print(f"      {status}: {count} ({percentage:.1f}%)")
            
            return df
            
    except Exception as e:
        print(f"   ‚ùå Error cargando datos: {e}")
        return pd.DataFrame()
        
    finally:
        driver.close()


def prepare_features_and_target(df):
    """
    Prepara features de emociones con one-hot encoding y target de viralidad.
    
    Args:
        df (pd.DataFrame): Datos de entrenamiento
        
    Returns:
        tuple: (X, y, feature_names)
    """
    print("\nüîß Preparando features de emociones y sentimientos...")
    
    # Crear DataFrame para features
    features_df = pd.DataFrame()
    
    # === ONE-HOT ENCODING PARA EMOCIONES ===
    print("   üò≠ Procesando emociones principales...")
    emotion_dummies = pd.get_dummies(df['emotion_label'], prefix='emotion')
    features_df = pd.concat([features_df, emotion_dummies], axis=1)
    print(f"      Emociones √∫nicas: {list(df['emotion_label'].unique())}")
    
    # === ONE-HOT ENCODING PARA SENTIMIENTOS ===
    print("   üòä Procesando sentimientos principales...")
    sentiment_dummies = pd.get_dummies(df['sentiment_label'], prefix='sentiment')
    features_df = pd.concat([features_df, sentiment_dummies], axis=1)
    print(f"      Sentimientos √∫nicos: {list(df['sentiment_label'].unique())}")
    
    # === SCORES NUM√âRICAS ===
    print("   üìä Agregando scores num√©ricas...")
    features_df['emotion_confidence'] = df['emotion_score']
    features_df['sentiment_confidence'] = df['sentiment_score']
    
    # === FEATURES DERIVADAS DE TODAS LAS EMOCIONES ===
    print("   üî¢ Calculando features derivadas...")
    
    # Procesar arrays de todas las emociones y sentimientos
    for idx, row in df.iterrows():
        # Emociones - crear features para cada emoci√≥n espec√≠fica
        if isinstance(row['emotion_all_labels'], list) and isinstance(row['emotion_all_scores'], list):
            emotion_dict = dict(zip(row['emotion_all_labels'], row['emotion_all_scores']))
            
            # Features espec√≠ficas por emoci√≥n
            for emotion in ['anger', 'fear', 'joy', 'love', 'sadness', 'surprise', 'optimism']:
                score = emotion_dict.get(emotion, 0.0)
                if f'emotion_score_{emotion}' not in features_df.columns:
                    features_df[f'emotion_score_{emotion}'] = 0.0
                features_df.at[idx, f'emotion_score_{emotion}'] = score
        
        # Sentimientos - crear features para cada sentimiento espec√≠fico  
        if isinstance(row['sentiment_all_labels'], list) and isinstance(row['sentiment_all_scores'], list):
            sentiment_dict = dict(zip(row['sentiment_all_labels'], row['sentiment_all_scores']))
            
            # Features espec√≠ficas por sentimiento
            for sentiment in ['negative', 'neutral', 'positive']:
                score = sentiment_dict.get(sentiment, 0.0)
                if f'sentiment_score_{sentiment}' not in features_df.columns:
                    features_df[f'sentiment_score_{sentiment}'] = 0.0
                features_df.at[idx, f'sentiment_score_{sentiment}'] = score
    
    # === FEATURES ESTAD√çSTICAS ===
    print("   üìà Calculando features estad√≠sticas...")
    
    # Entrop√≠a emocional (diversidad de emociones)
    emotion_entropy = []
    sentiment_entropy = []
    
    for idx, row in df.iterrows():
        # Entrop√≠a de emociones
        if isinstance(row['emotion_all_scores'], list):
            scores = np.array(row['emotion_all_scores'])
            scores = scores / scores.sum()  # Normalizar
            entropy = -np.sum(scores * np.log(scores + 1e-10))
            emotion_entropy.append(entropy)
        else:
            emotion_entropy.append(0.0)
            
        # Entrop√≠a de sentimientos
        if isinstance(row['sentiment_all_scores'], list):
            scores = np.array(row['sentiment_all_scores'])
            scores = scores / scores.sum()  # Normalizar
            entropy = -np.sum(scores * np.log(scores + 1e-10))
            sentiment_entropy.append(entropy)
        else:
            sentiment_entropy.append(0.0)
    
    features_df['emotion_entropy'] = emotion_entropy
    features_df['sentiment_entropy'] = sentiment_entropy
    
    # === TARGET ===
    y = df['is_viral'].astype(int)  # Convertir boolean a int
    
    # Rellenar valores faltantes
    features_df = features_df.fillna(0)
    
    print(f"   ‚úÖ Features preparadas: {features_df.shape[1]} columnas")
    print(f"   üìä Muestras: {len(features_df)}")
    print(f"   üéØ Target: {y.sum()} virales, {len(y) - y.sum()} no virales")
    
    return features_df.values, y.values, features_df.columns.tolist()


def train_and_evaluate_model(X, y, feature_names):
    """
    Entrena y eval√∫a modelo XGBoost con validaci√≥n cruzada.
    
    Args:
        X (np.array): Features
        y (np.array): Target
        feature_names (list): Nombres de features
        
    Returns:
        dict: Resultados del modelo
    """
    print("\nü§ñ Entrenando modelo XGBoost...")
    
    # Configurar XGBoost optimizado para clasificaci√≥n binaria
    xgb_model = xgb.XGBClassifier(
        n_estimators=400,            # Subir, porque bajamos el learning rate
        max_depth=3,                 # Simplificar √°rboles (fuerte regularizaci√≥n)
        learning_rate=0.05,          # Bajar la contribuci√≥n de cada √°rbol
        gamma=0.2,                   # A√±adir regularizaci√≥n (m√≠nima reducci√≥n de p√©rdida)
        min_child_weight=5,          # Evita nodos hoja muy espec√≠ficos
        subsample=0.7,               # Submuestreo de filas
        colsample_bytree=0.7,        # Submuestreo de columnas
        random_state=42,
        eval_metric='logloss',
        use_label_encoder=False
    )
    
    # === VALIDACI√ìN CRUZADA ===
    print("   üîÑ Realizando validaci√≥n cruzada (5-fold)...")
    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
    
    # M√©tricas m√∫ltiples
    cv_accuracy = cross_val_score(xgb_model, X, y, cv=cv, scoring='accuracy')
    cv_precision = cross_val_score(xgb_model, X, y, cv=cv, scoring='precision')
    cv_recall = cross_val_score(xgb_model, X, y, cv=cv, scoring='recall')
    cv_f1 = cross_val_score(xgb_model, X, y, cv=cv, scoring='f1')
    cv_roc_auc = cross_val_score(xgb_model, X, y, cv=cv, scoring='roc_auc')
    
    # === ENTRENAR MODELO FINAL ===
    print("   üéØ Entrenando modelo final...")
    xgb_model.fit(X, y)
    
    # Predicciones
    y_pred = xgb_model.predict(X)
    y_pred_proba = xgb_model.predict_proba(X)[:, 1]
    
    # === MOSTRAR RESULTADOS ===
    print(f"\nüìä RESULTADOS DE VALIDACI√ìN CRUZADA:")
    print(f"   üéØ Accuracy:  {cv_accuracy.mean():.4f} ¬± {cv_accuracy.std():.4f}")
    print(f"   üéØ Precision: {cv_precision.mean():.4f} ¬± {cv_precision.std():.4f}")
    print(f"   üéØ Recall:    {cv_recall.mean():.4f} ¬± {cv_recall.std():.4f}")
    print(f"   üéØ F1-Score:  {cv_f1.mean():.4f} ¬± {cv_f1.std():.4f}")
    print(f"   üéØ ROC-AUC:   {cv_roc_auc.mean():.4f} ¬± {cv_roc_auc.std():.4f}")
    
    # Reporte detallado en conjunto completo
    print(f"\nüìà REPORTE DETALLADO (Conjunto completo):")
    print(classification_report(y, y_pred, target_names=['No Viral', 'Viral']))
    
    return {
        'model': xgb_model,
        'cv_scores': {
            'accuracy': cv_accuracy,
            'precision': cv_precision,
            'recall': cv_recall,
            'f1': cv_f1,
            'roc_auc': cv_roc_auc
        },
        'predictions': y_pred,
        'probabilities': y_pred_proba,
        'true_labels': y
    }


def analyze_feature_importance(model_results, feature_names, df_train):
    """
    Analiza y visualiza la importancia de features emocionales.
    
    Args:
        model_results (dict): Resultados del modelo
        feature_names (list): Nombres de features
        df_train (pd.DataFrame): Datos de entrenamiento
    """
    print("\nüìä Analizando importancia de features emocionales...")
    
    # Obtener importancias
    model = model_results['model']
    importances = model.feature_importances_
    
    # Crear DataFrame de importancias
    importance_df = pd.DataFrame({
        'feature': feature_names,
        'importance': importances
    }).sort_values('importance', ascending=False)
    
    # Top 15 features m√°s importantes
    top_features = importance_df.head(15)
    
    print(f"   üèÜ TOP 15 FEATURES M√ÅS IMPORTANTES:")
    for idx, row in top_features.iterrows():
        print(f"      {row['feature']}: {row['importance']:.4f}")
    
    # === GR√ÅFICOS ===
    plt.figure(figsize=(15, 10))
    
    # 1. Importancia de features
    plt.subplot(2, 3, 1)
    plt.barh(range(len(top_features)), top_features['importance'])
    plt.yticks(range(len(top_features)), top_features['feature'])
    plt.xlabel('Importancia')
    plt.title('Top 15 Features m√°s Importantes')
    plt.gca().invert_yaxis()
    
    # 2. Distribuci√≥n de ROC-AUC en CV
    plt.subplot(2, 3, 2)
    cv_roc = model_results['cv_scores']['roc_auc']
    plt.hist(cv_roc, bins=5, alpha=0.7, edgecolor='black')
    plt.axvline(cv_roc.mean(), color='red', linestyle='--', 
                label=f'Media: {cv_roc.mean():.3f}')
    plt.xlabel('ROC-AUC Score')
    plt.ylabel('Frecuencia')
    plt.title('Distribuci√≥n ROC-AUC (5-Fold CV)')
    plt.legend()
    
    # 3. Curva ROC
    plt.subplot(2, 3, 3)
    y_true = model_results['true_labels']
    y_proba = model_results['probabilities']
    fpr, tpr, _ = roc_curve(y_true, y_proba)
    auc_score = roc_auc_score(y_true, y_proba)
    
    plt.plot(fpr, tpr, linewidth=2, label=f'ROC Curve (AUC = {auc_score:.3f})')
    plt.plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random Classifier')
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Curva ROC')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # 4. Matriz de Confusi√≥n
    plt.subplot(2, 3, 4)
    cm = confusion_matrix(y_true, model_results['predictions'])
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                xticklabels=['No Viral', 'Viral'],
                yticklabels=['No Viral', 'Viral'])
    plt.title('Matriz de Confusi√≥n')
    plt.ylabel('Etiqueta Real')
    plt.xlabel('Etiqueta Predicha')
    
    # 5. Distribuci√≥n de emociones principales
    plt.subplot(2, 3, 5)
    emotion_counts = df_train['emotion_label'].value_counts()
    plt.pie(emotion_counts.values, labels=emotion_counts.index, autopct='%1.1f%%')
    plt.title('Distribuci√≥n de Emociones Principales')
    
    # 6. Scores de confianza por viralidad
    plt.subplot(2, 3, 6)
    viral_data = df_train[df_train['is_viral'] == True]['emotion_score']
    non_viral_data = df_train[df_train['is_viral'] == False]['emotion_score']
    
    plt.hist(non_viral_data, bins=20, alpha=0.6, label='No Viral', color='blue')
    plt.hist(viral_data, bins=20, alpha=0.6, label='Viral', color='red')
    plt.xlabel('Confianza de Emoci√≥n')
    plt.ylabel('Frecuencia')
    plt.title('Distribuci√≥n de Confianza Emocional')
    plt.legend()
    
    plt.tight_layout()
    plt.savefig('emotion_prediction_analysis.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    print(f"   üíæ Gr√°fico guardado como 'emotion_prediction_analysis.png'")


def analyze_emotion_statistics(df_train):
    """
    Analiza estad√≠sticas descriptivas de emociones por viralidad.
    
    Args:
        df_train (pd.DataFrame): Datos de entrenamiento
    """
    print("\nüìà AN√ÅLISIS ESTAD√çSTICO DE EMOCIONES POR VIRALIDAD")
    print("=" * 60)
    
    # Separar por viralidad
    viral_df = df_train[df_train['is_viral'] == True]
    non_viral_df = df_train[df_train['is_viral'] == False]
    
    print(f"üìä Muestras analizadas:")
    print(f"   üî• Noticias virales: {len(viral_df)}")
    print(f"   üì∞ Noticias no virales: {len(non_viral_df)}")
    
    # === AN√ÅLISIS DE EMOCIONES PRINCIPALES ===
    print(f"\nüò≠ DISTRIBUCI√ìN DE EMOCIONES PRINCIPALES:")
    
    print(f"\n   üî• NOTICIAS VIRALES:")
    viral_emotions = viral_df['emotion_label'].value_counts(normalize=True) * 100
    for emotion, percentage in viral_emotions.items():
        print(f"      {emotion}: {percentage:.1f}%")
    
    print(f"\n   üì∞ NOTICIAS NO VIRALES:")
    non_viral_emotions = non_viral_df['emotion_label'].value_counts(normalize=True) * 100
    for emotion, percentage in non_viral_emotions.items():
        print(f"      {emotion}: {percentage:.1f}%")
    
    # === AN√ÅLISIS DE SENTIMIENTOS PRINCIPALES ===
    print(f"\nüòä DISTRIBUCI√ìN DE SENTIMIENTOS PRINCIPALES:")
    
    print(f"\n   üî• NOTICIAS VIRALES:")
    viral_sentiments = viral_df['sentiment_label'].value_counts(normalize=True) * 100
    for sentiment, percentage in viral_sentiments.items():
        print(f"      {sentiment}: {percentage:.1f}%")
    
    print(f"\n   üì∞ NOTICIAS NO VIRALES:")
    non_viral_sentiments = non_viral_df['sentiment_label'].value_counts(normalize=True) * 100
    for sentiment, percentage in non_viral_sentiments.items():
        print(f"      {sentiment}: {percentage:.1f}%")
    
    # === ESTAD√çSTICAS DE CONFIANZA ===
    print(f"\nüìä ESTAD√çSTICAS DE CONFIANZA:")
    
    print(f"\n   üò≠ CONFIANZA EMOCIONAL:")
    print(f"      Viral - Media: {viral_df['emotion_score'].mean():.4f}, "
          f"Std: {viral_df['emotion_score'].std():.4f}")
    print(f"      No Viral - Media: {non_viral_df['emotion_score'].mean():.4f}, "
          f"Std: {non_viral_df['emotion_score'].std():.4f}")
    
    print(f"\n   üòä CONFIANZA DE SENTIMIENTO:")
    print(f"      Viral - Media: {viral_df['sentiment_score'].mean():.4f}, "
          f"Std: {viral_df['sentiment_score'].std():.4f}")
    print(f"      No Viral - Media: {non_viral_df['sentiment_score'].mean():.4f}, "
          f"Std: {non_viral_df['sentiment_score'].std():.4f}")
    
    # === AN√ÅLISIS DE EMOCIONES ESPEC√çFICAS M√ÅS PREDICTIVAS ===
    print(f"\nüéØ EMOCIONES M√ÅS DIFERENCIADORAS:")
    
    # Calcular diferencias en distribuci√≥n
    differences = {}
    for emotion in df_train['emotion_label'].unique():
        viral_pct = (viral_df['emotion_label'] == emotion).mean() * 100
        non_viral_pct = (non_viral_df['emotion_label'] == emotion).mean() * 100
        difference = abs(viral_pct - non_viral_pct)
        differences[emotion] = {
            'viral_pct': viral_pct,
            'non_viral_pct': non_viral_pct,
            'difference': difference
        }
    
    # Ordenar por diferencia
    sorted_emotions = sorted(differences.items(), key=lambda x: x[1]['difference'], reverse=True)
    
    for emotion, stats in sorted_emotions:
        print(f"      {emotion}:")
        print(f"         Viral: {stats['viral_pct']:.1f}% | "
              f"No Viral: {stats['non_viral_pct']:.1f}% | "
              f"Diferencia: {stats['difference']:.1f}%")
    
    # === CONCLUSIONES ===
    print(f"\nüí° CONCLUSIONES PRELIMINARES:")
    
    best_emotion = sorted_emotions[0]
    cv_roc_mean = 0.5  # Placeholder - obtener del modelo real
    
    if hasattr(analyze_emotion_statistics, 'model_roc_auc'):
        cv_roc_mean = analyze_emotion_statistics.model_roc_auc
    
    print(f"   üìà La emoci√≥n m√°s diferenciadora es '{best_emotion[0]}' "
          f"(diferencia: {best_emotion[1]['difference']:.1f}%)")
    
    if cv_roc_mean > 0.7:
        print(f"   ‚úÖ Las emociones tienen BUENA capacidad predictiva (ROC-AUC > 0.7)")
    elif cv_roc_mean > 0.6:
        print(f"   ‚ö†Ô∏è Las emociones tienen capacidad predictiva MODERADA (ROC-AUC: 0.6-0.7)")
    else:
        print(f"   ‚ùå Las emociones tienen capacidad predictiva LIMITADA (ROC-AUC < 0.6)")
    
    print(f"   üé≠ Se recomienda combinar con otras features para mejor rendimiento")


if __name__ == '__main__':
    print("üî¨ AN√ÅLISIS DE PREDICCI√ìN DE VIRALIDAD BASADO EN EMOCIONES")
    print("=" * 80)
    print("üéØ Evaluando capacidad predictiva del an√°lisis emocional")
    print("ü§ñ Modelo: XGBoost con one-hot encoding y features derivadas")
    print("üìä M√©tricas: ROC-AUC, Precision, Recall, F1-Score")
    print("üé≠ Features: Emociones principales + scores + entrop√≠a emocional")
    print("=" * 80)
    
    analyze_emotion_prediction()
    
    print("\n" + "=" * 80)
    print("‚úÖ AN√ÅLISIS DE EMOCIONES COMPLETADO")
    print("üìä Resultados disponibles en:")
    print("   üìà M√©tricas de validaci√≥n cruzada impresas arriba")
    print("   üé® Gr√°fico: 'emotion_prediction_analysis.png'")
    print("   üìã Importancia de features y estad√≠sticas descriptivas")
    print("üí° Las emociones pueden ser √∫tiles como features complementarias")
    print("üöÄ Siguiente paso: Combinar con embeddings para mejor rendimiento")
    print("=" * 80)