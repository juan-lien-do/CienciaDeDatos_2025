from neo4j import GraphDatabase
import pandas as pd
import numpy as np
from sklearn.model_selection import cross_val_score, StratifiedKFold
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import roc_auc_score, f1_score
from sklearn.ensemble import IsolationForest
import xgboost as xgb
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings('ignore')

# Configuraci√≥n de Neo4j
NEO4J_URI = "neo4j://127.0.0.1:7687"
NEO4J_USER = "neo4j"
NEO4J_PASSWORD = "tu_password"  # CAMBIAR POR TU PASSWORD

# Configurar TensorFlow para usar GPU si est√° disponible
tf.config.experimental.set_memory_growth(tf.config.experimental.list_physical_devices('GPU')[0] if tf.config.experimental.list_physical_devices('GPU') else None, True) if tf.config.experimental.list_physical_devices('GPU') else None


def build_autoencoder():
    """
    Construye un autoencoder para reducir embeddings de 384D a 64D.
    
    Arquitectura:
    - Entrada: 384 dimensiones
    - Encoder: Dense(128, relu) -> Dropout(0.2) -> Dense(64, relu) [Capa Latente]
    - Decoder: Dense(128, relu) -> Dense(384, linear)
    
    Returns:
        tuple: (autoencoder_completo, encoder_solo)
    """
    print("üß† Construyendo arquitectura del Autoencoder...")
    
    # === ENCODER ===
    input_layer = layers.Input(shape=(384,), name='input_embeddings')
    
    # Primera capa del encoder
    encoded = layers.Dense(128, activation='relu', name='encoder_dense_1')(input_layer)
    encoded = layers.Dropout(0.2, name='encoder_dropout')(encoded)
    
    # Capa latente (bottleneck)
    latent = layers.Dense(64, activation='relu', name='latent_space')(encoded)
    
    # === DECODER ===
    decoded = layers.Dense(128, activation='relu', name='decoder_dense_1')(latent)
    output_layer = layers.Dense(384, activation='linear', name='reconstructed_embeddings')(decoded)
    
    # === MODELOS ===
    # Autoencoder completo (entrada -> reconstrucci√≥n)
    autoencoder = keras.Model(input_layer, output_layer, name='autoencoder')
    
    # Encoder solo (entrada -> representaci√≥n latente)
    encoder = keras.Model(input_layer, latent, name='encoder')
    
    # Compilar autoencoder
    autoencoder.compile(
        optimizer='adam',
        loss='mse',
        metrics=['mae']
    )
    
    print("   ‚úÖ Autoencoder construido:")
    print(f"      üì• Entrada: 384 dimensiones (embeddings)")
    print(f"      üîÑ Capa latente: 64 dimensiones")
    print(f"      üì§ Salida: 384 dimensiones (reconstrucci√≥n)")
    print(f"      üéØ Optimizador: Adam | Loss: MSE")
    
    return autoencoder, encoder


def load_training_data_with_embeddings():
    """
    Carga datos de entrenamiento incluyendo embeddings limpios y an√°lisis emocional.
    
    Returns:
        pd.DataFrame: Datos completos de entrenamiento
    """
    print("\nüìä Cargando datos con embeddings y emociones...")
    
    driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASSWORD))
    
    try:
        with driver.session() as session:
            query = """
                MATCH (n:Noticia)
                WHERE n.subset = 'train'
                  AND n.popularity IS NOT NULL
                  AND n.embedding_titulo_clean IS NOT NULL
                  AND n.analisis_sentimiento_titulo_label IS NOT NULL
                  AND n.analisis_emocion_titulo_label IS NOT NULL
                RETURN 
                    n.titulo AS titulo,
                    n.popularity AS is_viral,
                    n.embedding_titulo_clean AS embeddings,
                    n.analisis_sentimiento_titulo_label AS sentiment_label,
                    n.analisis_sentimiento_titulo_score AS sentiment_score,
                    n.analisis_sentimiento_titulo_all_labels AS sentiment_all_labels,
                    n.analisis_sentimiento_titulo_all_scores AS sentiment_all_scores,
                    n.analisis_emocion_titulo_label AS emotion_label,
                    n.analisis_emocion_titulo_score AS emotion_score,
                    n.analisis_emocion_titulo_all_labels AS emotion_all_labels,
                    n.analisis_emocion_titulo_all_scores AS emotion_all_scores
            """
            
            result = session.run(query)
            records = [dict(record) for record in result]
            
            if not records:
                print("   ‚ùå No se encontraron datos con embeddings limpios y emociones")
                return pd.DataFrame()
            
            df = pd.DataFrame(records)
            
            print(f"   ‚úÖ Cargados {len(df)} registros completos")
            print(f"   üìà Distribuci√≥n de viralidad: {df['is_viral'].value_counts().to_dict()}")
            
            return df
            
    except Exception as e:
        print(f"   ‚ùå Error cargando datos: {e}")
        return pd.DataFrame()
        
    finally:
        driver.close()


def prepare_features_for_hybrid_model(df):
    """
    Prepara embeddings y features emocionales para el modelo h√≠brido.
    
    Args:
        df (pd.DataFrame): Datos de entrenamiento
        
    Returns:
        tuple: (X_embeddings, X_emotion_features, y, emotion_feature_names)
    """
    print("\nüîß Preparando features para modelo h√≠brido...")
    
    # === EMBEDDINGS (384D) ===
    print("   üìä Extrayendo embeddings de 384D...")
    embeddings_list = []
    valid_indices = []
    
    for idx, row in df.iterrows():
        if isinstance(row['embeddings'], list) and len(row['embeddings']) == 384:
            embeddings_list.append(row['embeddings'])
            valid_indices.append(idx)
    
    if not embeddings_list:
        raise ValueError("No se encontraron embeddings v√°lidos de 384D")
    
    X_embeddings = np.array(embeddings_list)
    
    # Filtrar DataFrame solo con √≠ndices v√°lidos
    df_valid = df.iloc[valid_indices].reset_index(drop=True)
    
    print(f"      ‚úÖ Embeddings: {X_embeddings.shape}")
    
    # === FEATURES EMOCIONALES ===
    print("   üé≠ Preparando features emocionales...")
    emotion_df = pd.DataFrame()
    
    # One-hot encoding para emociones y sentimientos principales
    emotion_dummies = pd.get_dummies(df_valid['emotion_label'], prefix='emotion')
    sentiment_dummies = pd.get_dummies(df_valid['sentiment_label'], prefix='sentiment')
    
    emotion_df = pd.concat([emotion_df, emotion_dummies, sentiment_dummies], axis=1)
    
    # Scores de confianza
    emotion_df['emotion_confidence'] = df_valid['emotion_score'].values
    emotion_df['sentiment_confidence'] = df_valid['sentiment_score'].values
    
    # Features espec√≠ficas por emoci√≥n y sentimiento
    for idx, row in df_valid.iterrows():
        # Emociones espec√≠ficas
        if isinstance(row['emotion_all_labels'], list) and isinstance(row['emotion_all_scores'], list):
            emotion_dict = dict(zip(row['emotion_all_labels'], row['emotion_all_scores']))
            
            for emotion in ['anger', 'fear', 'joy', 'love', 'sadness', 'surprise', 'optimism']:
                score = emotion_dict.get(emotion, 0.0)
                col_name = f'emotion_score_{emotion}'
                if col_name not in emotion_df.columns:
                    emotion_df[col_name] = 0.0
                emotion_df.at[idx, col_name] = score
        
        # Sentimientos espec√≠ficos
        if isinstance(row['sentiment_all_labels'], list) and isinstance(row['sentiment_all_scores'], list):
            sentiment_dict = dict(zip(row['sentiment_all_labels'], row['sentiment_all_scores']))
            
            for sentiment in ['negative', 'neutral', 'positive']:
                score = sentiment_dict.get(sentiment, 0.0)
                col_name = f'sentiment_score_{sentiment}'
                if col_name not in emotion_df.columns:
                    emotion_df[col_name] = 0.0
                emotion_df.at[idx, col_name] = score
    
    # Rellenar valores faltantes
    emotion_df = emotion_df.fillna(0)
    X_emotion_features = emotion_df.values
    
    # Target
    y = df_valid['is_viral'].astype(int).values
    
    print(f"      ‚úÖ Features emocionales: {X_emotion_features.shape}")
    print(f"      üéØ Target: {y.sum()} virales / {len(y)} total")
    
    return X_embeddings, X_emotion_features, y, emotion_df.columns.tolist()


def clean_emotion_features_iqr(X_emotion):
    """
    Aplica Winsorization basada en IQR a las features emocionales.
    
    Args:
        X_emotion (np.array): Matriz de features emocionales
        
    Returns:
        np.array: Features emocionales con outliers truncados
    """
    print("   üìê Calculando l√≠mites IQR para cada feature emocional...")
    
    X_emotion_df = pd.DataFrame(X_emotion)
    X_emotion_clean = X_emotion_df.copy()
    
    outliers_count = 0
    
    for col in X_emotion_df.columns:
        # Calcular Q1, Q3 e IQR
        Q1 = X_emotion_df[col].quantile(0.25)
        Q3 = X_emotion_df[col].quantile(0.75)
        IQR = Q3 - Q1
        
        # L√≠mites de Winsorization (1.5 * IQR)
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR
        
        # Contar outliers antes de truncar
        outliers_before = ((X_emotion_df[col] < lower_bound) | (X_emotion_df[col] > upper_bound)).sum()
        outliers_count += outliers_before
        
        # Aplicar Winsorization (truncamiento)
        X_emotion_clean[col] = X_emotion_df[col].clip(lower=lower_bound, upper=upper_bound)
    
    print(f"      ‚úÇÔ∏è Features emocionales truncadas: {outliers_count} valores at√≠picos")
    
    return X_emotion_clean.values


def detect_embedding_outliers(X_embed, contamination=0.01):
    """
    Detecta outliers en embeddings usando Isolation Forest.
    
    Args:
        X_embed (np.array): Matriz de embeddings (384D)
        contamination (float): Proporci√≥n esperada de outliers
        
    Returns:
        np.array: M√°scara booleana (True = inlier, False = outlier)
    """
    print(f"   üå≤ Configurando Isolation Forest (contamination={contamination})...")
    
    # Configurar Isolation Forest
    iso_forest = IsolationForest(
        contamination=contamination,
        random_state=42,
        n_jobs=-1
    )
    
    # Ajustar y predecir outliers
    outlier_predictions = iso_forest.fit_predict(X_embed)
    
    # Convertir a m√°scara booleana (1 = inlier, -1 = outlier)
    inlier_mask = outlier_predictions == 1
    
    outliers_detected = (~inlier_mask).sum()
    outlier_percentage = (outliers_detected / len(X_embed)) * 100
    
    print(f"      üîç Outliers detectados: {outliers_detected} ({outlier_percentage:.2f}%)")
    print(f"      ‚úÖ Inliers preservados: {inlier_mask.sum()} ({(inlier_mask.sum()/len(X_embed)*100):.2f}%)")
    
    return inlier_mask


def train_viral_prediction_model(df_train):
    """
    Entrena modelo h√≠brido Autoencoder + XGBoost para predicci√≥n de viralidad.
    
    Flujo:
    1. Separa embeddings (384D) y features emocionales
    2. NUEVO: Limpia outliers con IQR + Isolation Forest
    3. Entrena autoencoder para reducir embeddings a 64D
    4. Combina features latentes (64D) con emocionales limpias
    5. Entrena XGBoost altamente regularizado
    6. Eval√∫a con validaci√≥n cruzada estratificada
    
    Args:
        df_train (pd.DataFrame): Datos de entrenamiento
    """
    print("üöÄ ENTRENANDO MODELO H√çBRIDO AUTOENCODER + XGBOOST")
    print("=" * 80)
    print("üéØ Objetivo: Predicir viralidad con embeddings reducidos + emociones")
    print("üß† Pipeline: Embeddings(384D) -> Autoencoder(64D) -> XGBoost")
    print("üìä Validaci√≥n: 5-fold cross-validation estratificada")
    print("=" * 80)
    
    # a. Preparar datos
    X_embed, X_emotion, y, emotion_names = prepare_features_for_hybrid_model(df_train)
    
    # === FASE DE LIMPIEZA DE OUTLIERS ===
    print("\nüßπ LIMPIEZA DE OUTLIERS")
    print("=" * 50)
    
    # 1. TRATAMIENTO IQR para X_emotion (Winsorization)
    print("üìä Aplicando Winsorization IQR a features emocionales...")
    X_emotion_clean = clean_emotion_features_iqr(X_emotion)
    
    # 2. TRATAMIENTO ISOLATION FOREST para X_embed
    print("üå≤ Aplicando Isolation Forest a embeddings...")
    inlier_mask = detect_embedding_outliers(X_embed)
    
    # 3. FILTRADO FINAL
    print("üîç Aplicando filtrado final con m√°scara de inliers...")
    X_embed = X_embed[inlier_mask]
    X_emotion = X_emotion_clean[inlier_mask]
    y = y[inlier_mask]
    
    print(f"   ‚úÖ Datos despu√©s de limpieza:")
    print(f"      üìä Muestras restantes: {len(X_embed)} (eliminadas: {(~inlier_mask).sum()})")
    print(f"      üéØ Distribuci√≥n viral: {y.sum()} virales / {len(y)} total")
    
    # Normalizar embeddings para el autoencoder
    print("\nüìè Normalizando embeddings...")
    scaler = StandardScaler()
    X_embed_scaled = scaler.fit_transform(X_embed)
    
    # b. Construir y entrenar autoencoder
    print("\nüîÑ Entrenando Autoencoder...")
    autoencoder, encoder = build_autoencoder()
    
    # Entrenar autoencoder (40 epochs)
    print("   üéØ Entrenando por 40 epochs...")
    history = autoencoder.fit(
        X_embed_scaled, X_embed_scaled,
        epochs=40,
        batch_size=32,
        validation_split=0.2,
        verbose=1,
        shuffle=True
    )
    
    # c. Usar encoder para obtener representaci√≥n latente
    print("\nüß¨ Generando representaci√≥n latente (64D)...")
    X_latent = encoder.predict(X_embed_scaled, verbose=0)
    print(f"   ‚úÖ Features latentes: {X_latent.shape}")
    
    # d. Combinar features latentes con emocionales
    print("\nüîó Combinando features latentes + emocionales...")
    X_final = np.concatenate([X_latent, X_emotion], axis=1)
    print(f"   ‚úÖ Features finales: {X_final.shape} ({X_latent.shape[1]} latentes + {X_emotion.shape[1]} emocionales)")
    
    # e. Definir XGBoost altamente regularizado
    print("\nü§ñ Configurando XGBoost altamente regularizado...")
    xgb_model = xgb.XGBClassifier(
        n_estimators=300,           # Muchos √°rboles pero con learning rate bajo
        max_depth=4,                # √Årboles poco profundos
        learning_rate=0.3,         # Learning rate muy conservador
        gamma=0.5,                  # Alta regularizaci√≥n (min split loss)
        min_child_weight=10,        # Evita hojas con pocas muestras
        subsample=0.7,              # Submuestreo de filas
        colsample_bytree=0.7,       # Submuestreo de columnas
        reg_alpha=0.1,              # Regularizaci√≥n L1
        random_state=42,
        eval_metric='logloss',
        use_label_encoder=False,
        n_jobs=-1                   # Usar todos los cores
    )
    
    print("   ‚úÖ XGBoost configurado con alta regularizaci√≥n")
    
    # f. Validaci√≥n cruzada estratificada
    print("\nüìä Realizando validaci√≥n cruzada estratificada (5-fold)...")
    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
    
    # Calcular m√©tricas
    cv_roc_auc = cross_val_score(xgb_model, X_final, y, cv=cv, scoring='roc_auc', n_jobs=-1)
    cv_f1 = cross_val_score(xgb_model, X_final, y, cv=cv, scoring='f1', n_jobs=-1)
    
    # g. Imprimir resultados
    print("\nüéØ RESULTADOS DE VALIDACI√ìN CRUZADA:")
    print("=" * 50)
    print(f"üìä ROC-AUC: {cv_roc_auc.mean():.4f} ¬± {cv_roc_auc.std():.4f}")
    print(f"üìä F1-Score: {cv_f1.mean():.4f} ¬± {cv_f1.std():.4f}")
    
    # Mostrar resultados por fold
    print(f"\nüìã Resultados detallados por fold:")
    for i, (roc, f1) in enumerate(zip(cv_roc_auc, cv_f1), 1):
        print(f"   Fold {i}: ROC-AUC={roc:.4f}, F1={f1:.4f}")
    
    # Entrenar modelo final para an√°lisis
    print(f"\nüéØ Entrenando modelo final para an√°lisis de features...")
    xgb_model.fit(X_final, y)
    
    # Analizar importancia de features
    analyze_feature_importance_hybrid(xgb_model, X_latent.shape[1], emotion_names)
    
    # Mostrar p√©rdida del autoencoder
    plot_autoencoder_loss(history)
    
    return {
        'cv_roc_auc': cv_roc_auc,
        'cv_f1': cv_f1,
        'autoencoder': autoencoder,
        'encoder': encoder,
        'xgb_model': xgb_model,
        'scaler': scaler
    }


def analyze_feature_importance_hybrid(model, n_latent_features, emotion_names):
    """
    Analiza la importancia de features en el modelo h√≠brido.
    
    Args:
        model: Modelo XGBoost entrenado
        n_latent_features (int): N√∫mero de features latentes
        emotion_names (list): Nombres de features emocionales
    """
    print(f"\nüîç AN√ÅLISIS DE IMPORTANCIA DE FEATURES")
    print("=" * 50)
    
    importances = model.feature_importances_
    
    # Separar importancias
    latent_importances = importances[:n_latent_features]
    emotion_importances = importances[n_latent_features:]
    
    print(f"üìä Features Latentes (Autoencoder):")
    print(f"   üß¨ Promedio: {latent_importances.mean():.4f}")
    print(f"   üìà Std: {latent_importances.std():.4f}")
    print(f"   üèÜ M√°s importante: Latente-{latent_importances.argmax()} ({latent_importances.max():.4f})")
    
    print(f"\nüé≠ Features Emocionales:")
    if len(emotion_importances) > 0:
        top_emotion_indices = emotion_importances.argsort()[-5:][::-1]
        print(f"   üèÜ Top 5 features emocionales:")
        for i, idx in enumerate(top_emotion_indices, 1):
            if idx < len(emotion_names):
                print(f"      {i}. {emotion_names[idx]}: {emotion_importances[idx]:.4f}")
    
    # Comparaci√≥n general
    latent_total = latent_importances.sum()
    emotion_total = emotion_importances.sum()
    total = latent_total + emotion_total
    
    print(f"\n‚öñÔ∏è CONTRIBUCI√ìN TOTAL:")
    print(f"   üß¨ Features Latentes: {latent_total:.3f} ({latent_total/total*100:.1f}%)")
    print(f"   üé≠ Features Emocionales: {emotion_total:.3f} ({emotion_total/total*100:.1f}%)")


def plot_autoencoder_loss(history):
    """
    Visualiza la p√©rdida del autoencoder durante el entrenamiento.
    
    Args:
        history: Historia del entrenamiento de Keras
    """
    plt.figure(figsize=(12, 4))
    
    # P√©rdida de entrenamiento y validaci√≥n
    plt.subplot(1, 2, 1)
    plt.plot(history.history['loss'], label='Entrenamiento')
    plt.plot(history.history['val_loss'], label='Validaci√≥n')
    plt.title('P√©rdida del Autoencoder')
    plt.xlabel('Epoch')
    plt.ylabel('MSE Loss')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # MAE (Mean Absolute Error)
    plt.subplot(1, 2, 2)
    plt.plot(history.history['mae'], label='MAE Entrenamiento')
    plt.plot(history.history['val_mae'], label='MAE Validaci√≥n')
    plt.title('Error Absoluto Medio')
    plt.xlabel('Epoch')
    plt.ylabel('MAE')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('autoencoder_training.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    print(f"   üíæ Gr√°fico guardado como 'autoencoder_training.png'")


def main():
    """Funci√≥n principal que ejecuta todo el pipeline."""
    print("üéØ MODELO H√çBRIDO: AUTOENCODER + XGBOOST PARA PREDICCI√ìN DE VIRALIDAD")
    print("=" * 80)
    print("üß† Arquitectura: Embeddings(384D) -> Autoencoder(64D) + Emociones -> XGBoost")
    print("üé≠ Features: T√≠tulos limpios + an√°lisis emocional completo")
    print("üî¨ Validaci√≥n: Cross-validation estratificada con alta regularizaci√≥n")
    print("=" * 80)
    
    # Cargar datos
    df_train = load_training_data_with_embeddings()
    
    if df_train.empty:
        print("‚ùå No se pudieron cargar los datos de entrenamiento")
        return
    
    # Entrenar modelo h√≠brido
    results = train_viral_prediction_model(df_train)
    
    print("\n" + "=" * 80)
    print("‚úÖ ENTRENAMIENTO COMPLETADO")
    print("üìä Resultados:")
    print(f"   üéØ ROC-AUC medio: {results['cv_roc_auc'].mean():.4f}")
    print(f"   üéØ F1-Score medio: {results['cv_f1'].mean():.4f}")
    print("üìÅ Artefactos generados:")
    print("   üé® autoencoder_training.png: Curvas de entrenamiento")
    print("   üìä An√°lisis de importancia de features")
    print("üí° El modelo combina representaciones latentes con an√°lisis emocional")
    print("=" * 80)


if __name__ == '__main__':
    main()